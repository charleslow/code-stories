{
  "id": "c772fcfe-fd5d-4dee-a392-1eb6c92dbb4b",
  "title": "nanoGPT: A Language Model from Scratch",
  "query": "Walk me through nanogpt, assume that I know nothing about LLMs.",
  "repo": "karpathy/nanoGPT",
  "commitHash": "3adf61e154c3fe3fca428ad6bc3818b27a3b8291",
  "createdAt": "2026-02-10T12:00:00.000Z",
  "chapters": [
    {
      "id": "chapter-0",
      "label": "The Big Picture",
      "snippets": [],
      "explanation": "nanoGPT is a minimal, from-scratch implementation of GPT — the architecture behind ChatGPT and similar AI systems. Written by Andrej Karpathy, the entire project fits in about 300 lines of core code across just a few files. Its goal: make the magic of large language models (LLMs) accessible and hackable.\n\nSo what does a language model actually do? At its core, it predicts the next word (or character) given everything that came before. Feed it \"To be or not to\" and it learns to predict \"be.\" Stack enough of these predictions together and you get surprisingly coherent text generation.\n\nA few key terms you'll encounter: **tokens** are the atomic units of text (characters or word-pieces) converted to numbers. **Embeddings** turn those numbers into rich vectors the model can reason about. **Attention** lets the model decide which earlier tokens matter for predicting the next one. **Transformer blocks** combine attention with simple neural networks, stacked repeatedly. **Training** adjusts the model's millions of parameters to minimize prediction errors. **Generation** runs the model in a loop, producing one new token at a time.\n\nThe codebase has four main files: `model.py` (the architecture), `train.py` (the training loop), `sample.py` (text generation), and `configurator.py` (settings). Let's start where every language model begins — with turning text into numbers."
    },
    {
      "id": "chapter-1",
      "label": "Text to Numbers",
      "snippets": [
        {
          "filePath": "data/shakespeare_char/prepare.py",
          "startLine": 23,
          "endLine": 35,
          "content": "# get all the unique characters that occur in this text\nchars = sorted(list(set(data)))\nvocab_size = len(chars)\nprint(\"all the unique characters:\", ''.join(chars))\nprint(f\"vocab size: {vocab_size:,}\")\n\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\ndef encode(s):\n    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndef decode(l):\n    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
        },
        {
          "filePath": "data/shakespeare_char/prepare.py",
          "startLine": 37,
          "endLine": 52,
          "content": "# create the train and test splits\nn = len(data)\ntrain_data = data[:int(n*0.9)]\nval_data = data[int(n*0.9):]\n\n# encode both to integers\ntrain_ids = encode(train_data)\nval_ids = encode(val_data)\nprint(f\"train has {len(train_ids):,} tokens\")\nprint(f\"val has {len(val_ids):,} tokens\")\n\n# export to bin files\ntrain_ids = np.array(train_ids, dtype=np.uint16)\nval_ids = np.array(val_ids, dtype=np.uint16)\ntrain_ids.tofile(os.path.join(os.path.dirname(__file__), 'train.bin'))\nval_ids.tofile(os.path.join(os.path.dirname(__file__), 'val.bin'))"
        }
      ],
      "explanation": "Before a model can process text, every character needs a numeric identity. Lines 24-25 extract all unique characters from Shakespeare's works and count them — there are only 65 (letters, punctuation, and a few special characters). Lines 30-31 create two dictionaries: `stoi` maps each character to an integer (\"string to integer\"), and `itos` does the reverse. The `encode` function on line 32 converts any string into a list of integers, while `decode` on line 34 converts back.\n\nThe second snippet shows what happens next: the text is split 90/10 into training and validation sets (lines 38-40), encoded into integer arrays (lines 43-44), and saved as raw binary files (lines 49-52). The choice of `np.uint16` on line 49 is deliberate — with only 65 possible values, 16-bit integers are more than sufficient, and this compact format keeps the data files small. These `.bin` files become the input that the training script reads directly from disk."
    },
    {
      "id": "chapter-2",
      "label": "Model Blueprint",
      "snippets": [
        {
          "filePath": "model.py",
          "startLine": 108,
          "endLine": 116,
          "content": "@dataclass\nclass GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    dropout: float = 0.0\n    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
        }
      ],
      "explanation": "`GPTConfig` defines the entire model with just seven numbers. `block_size` (line 110) sets how many tokens the model can look back at — its \"memory window.\" `n_layer`, `n_head`, and `n_embd` (lines 112-114) control depth, parallel attention heads, and the width of internal representations, respectively. Notice the comment on line 111: the vocab size is padded from 50257 to 50304 so it's divisible by 64, a small trick that aligns matrix dimensions to GPU tensor cores for faster computation."
    },
    {
      "id": "chapter-3",
      "label": "Assembling the GPT",
      "snippets": [
        {
          "filePath": "model.py",
          "startLine": 118,
          "endLine": 145,
          "content": "class GPT(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.dropout),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # with weight tying when using torch.compile() some warnings get generated:\n        # \"UserWarning: functional_call was passed multiple values for tied weights.\n        # This behavior is deprecated and will be an error in future versions\"\n        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n\n        # init all weights\n        self.apply(self._init_weights)\n        # apply special scaled init to the residual projections, per GPT-2 paper\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))"
        }
      ],
      "explanation": "The GPT constructor wires together every component we'll explore in the following chapters. The `transformer` ModuleDict (lines 126-132) is essentially a named parts list: `wte` embeds tokens into vectors, `wpe` adds positional information (so the model knows *where* each token sits in the sequence), `h` is a list of transformer blocks (the repeating core of the architecture), and `ln_f` is a final normalization layer.\n\nLine 133 creates `lm_head`, a linear layer that projects the model's internal representations back to vocabulary-sized logits — essentially scores for how likely each token is to come next. But the really clever move is line 138: **weight tying**. The same weight matrix used to embed tokens into vectors (`wte`) is shared with `lm_head`, which converts vectors back to token predictions. This cuts the parameter count significantly and actually improves performance — the intuition being that the \"meaning\" of a token should be consistent whether you're reading it or predicting it.\n\nLines 141-145 handle weight initialization. Most weights get a normal distribution with std=0.02, but residual projection layers (`c_proj`) use a scaled-down std of `0.02/sqrt(2*n_layer)`. This prevents signal variance from growing uncontrollably as data passes through many stacked layers."
    },
    {
      "id": "chapter-4",
      "label": "The Transformer Block",
      "snippets": [
        {
          "filePath": "model.py",
          "startLine": 78,
          "endLine": 106,
          "content": "class MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.gelu    = nn.GELU()\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x"
        }
      ],
      "explanation": "Every transformer is built by repeating one fundamental unit: the `Block`. Before diving into it, look at the `MLP` class (lines 78-92). It's a simple two-layer network: expand the input to 4x its size (line 82), apply the GELU activation function (line 83), then project back down (line 84). This expansion-and-compression pattern gives the model room to learn complex transformations while keeping the overall dimension manageable.\n\nThe `Block` class (lines 94-106) combines attention and MLP with two important patterns. First, **pre-normalization**: inputs are normalized *before* each sub-layer (`self.ln_1` and `self.ln_2`) rather than after. This was found to stabilize training. Second, **residual connections**: lines 104-105 add the sub-layer's output back to its input (`x = x + ...`). This lets gradients flow directly through the network during training and means each block only needs to learn *refinements* to the representation, not rebuild it from scratch."
    },
    {
      "id": "chapter-5",
      "label": "How Attention Works",
      "snippets": [
        {
          "filePath": "model.py",
          "startLine": 29,
          "endLine": 50,
          "content": "class CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n        if not self.flash:\n            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n            # causal mask to ensure that attention is only applied to the left in the input sequence\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                        .view(1, 1, config.block_size, config.block_size))"
        },
        {
          "filePath": "model.py",
          "startLine": 52,
          "endLine": 76,
          "content": "    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        if self.flash:\n            # efficient attention using Flash Attention CUDA kernels\n            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n        else:\n            # manual implementation of attention\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y"
        }
      ],
      "explanation": "`CausalSelfAttention` is the mechanism that lets each token \"look at\" other tokens to understand context. Line 35 creates a single linear layer that produces three outputs simultaneously: queries (Q), keys (K), and values (V) — packed together as `3 * n_embd` for efficiency rather than using three separate layers.\n\nThe \"causal\" in the name comes from lines 46-50: when Flash Attention isn't available, a triangular mask is registered as a buffer. This mask ensures that position 5, for example, can attend to positions 0-4 but not positions 6 onward. Without this constraint, the model would \"cheat\" during training by seeing future tokens it's supposed to predict.\n\nIn the forward method, line 56 splits the combined projection into separate Q, K, V tensors, then lines 57-59 reshape them for multi-head attention. The key idea: instead of one large attention operation, the embedding is divided across `n_head` independent \"heads\" (each of size `n_embd // n_head`). Each head can focus on different relationships — one might track grammar, another might track topic coherence.\n\nLines 62-71 compute the actual attention. With Flash Attention (line 64), this is a single highly optimized CUDA kernel call. The manual fallback (lines 67-71) shows the math explicitly: Q and K are multiplied to produce attention scores, scaled by `1/sqrt(head_size)` to prevent large values from saturating the softmax, masked to enforce causality, then used to create a weighted combination of V. Finally, line 72 reassembles all heads and line 75 applies the output projection."
    },
    {
      "id": "chapter-6",
      "label": "Forward Pass",
      "snippets": [
        {
          "filePath": "model.py",
          "startLine": 170,
          "endLine": 193,
          "content": "    def forward(self, idx, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n\n        # forward the GPT model itself\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n\n        if targets is not None:\n            # if we are given some desired targets also calculate the loss\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n        else:\n            # inference-time mini-optimization: only forward the lm_head on the very last position\n            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n            loss = None\n\n        return logits, loss"
        }
      ],
      "explanation": "Here is where a sequence of token IDs becomes predictions. Lines 171-174 set up the inputs: `idx` contains the token indices, and `pos` creates a matching sequence of position numbers (0, 1, 2, ...).\n\nLines 177-179 are the model's entry point: token embeddings and position embeddings are looked up and summed together. Adding position embeddings is how the model knows token order — without them, \"the cat sat\" and \"sat cat the\" would look identical. Dropout is applied for regularization during training.\n\nLines 180-182 run the data through every transformer block in sequence, then apply final layer normalization. The real branching logic starts at line 184: during training (when targets are provided), `lm_head` produces logits for every position and cross-entropy loss is computed. During inference (line 190), an optimization kicks in — the model only computes logits for the *last* position, since that's the only one we need for generating the next token. This small detail makes generation meaningfully faster."
    },
    {
      "id": "chapter-7",
      "label": "Loading Data",
      "snippets": [
        {
          "filePath": "train.py",
          "startLine": 114,
          "endLine": 131,
          "content": "# poor man's data loader\ndata_dir = os.path.join('data', dataset)\ndef get_batch(split):\n    # We recreate np.memmap every batch to avoid a memory leak, as per\n    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n    if split == 'train':\n        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n    else:\n        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n    if device_type == 'cuda':\n        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n    else:\n        x, y = x.to(device), y.to(device)\n    return x, y"
        }
      ],
      "explanation": "Karpathy's self-described \"poor man's data loader\" (lines 114-131) skips PyTorch's `DataLoader` class entirely. The function uses `np.memmap` to memory-map the binary files created during data preparation — the OS handles paging data in and out of RAM on demand, so even multi-gigabyte datasets don't need to fit in memory.\n\nLine 123 picks random starting positions, then lines 124-125 create input-target pairs: `x` is a sequence of tokens and `y` is the same sequence shifted by one position. This is the fundamental setup for next-token prediction — the model sees tokens 0 through N-1 and tries to predict tokens 1 through N. Lines 126-128 use `pin_memory()` for faster CPU-to-GPU transfers."
    },
    {
      "id": "chapter-8",
      "label": "The Training Loop",
      "snippets": [
        {
          "filePath": "train.py",
          "startLine": 290,
          "endLine": 314,
          "content": "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n    # and using the GradScaler if data type is float16\n    for micro_step in range(gradient_accumulation_steps):\n        if ddp:\n            # in DDP training we only need to sync gradients at the last micro step.\n            # the official way to do this is with model.no_sync() context manager, but\n            # I really dislike that this bloats the code and forces us to repeat code\n            # looking at the source of that context manager, it just toggles this variable\n            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n        with ctx:\n            logits, loss = model(X, Y)\n            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n        X, Y = get_batch('train')\n        # backward pass, with gradient scaling if training in fp16\n        scaler.scale(loss).backward()\n    # clip the gradient\n    if grad_clip != 0.0:\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n    # step the optimizer and scaler if training in fp16\n    scaler.step(optimizer)\n    scaler.update()\n    # flush the gradients as soon as we can, no need for this memory anymore\n    optimizer.zero_grad(set_to_none=True)"
        }
      ],
      "explanation": "These 25 lines are the engine that actually teaches the model to predict text. The outer loop (line 292) implements **gradient accumulation**: instead of updating weights after every batch, it accumulates gradients across multiple micro-steps. This simulates a larger batch size when GPU memory is limited. Line 301 scales the loss by `1/gradient_accumulation_steps` so the accumulated gradients have the correct magnitude.\n\nLines 293-298 handle a distributed training optimization. When training across multiple GPUs, gradients need to be synchronized — but only on the last micro-step. Rather than using PyTorch's `no_sync()` context manager, Karpathy directly toggles the `require_backward_grad_sync` flag, calling the official approach \"code bloat.\"\n\nLine 299 wraps the forward pass in `ctx` — the mixed-precision autocast context that runs computations in float16 or bfloat16 for speed. Line 303 prefetches the next batch while the GPU is busy. Lines 305-312 handle the backward pass: `scaler.scale(loss).backward()` applies gradient scaling (necessary for float16 to prevent underflow), then gradients are clipped (lines 307-309) and the optimizer steps. Line 314's `set_to_none=True` frees gradient memory immediately rather than zeroing it — a small but meaningful optimization."
    },
    {
      "id": "chapter-9",
      "label": "Learning Rate Schedule",
      "snippets": [
        {
          "filePath": "train.py",
          "startLine": 230,
          "endLine": 242,
          "content": "# learning rate decay scheduler (cosine with warmup)\ndef get_lr(it):\n    # 1) linear warmup for warmup_iters steps\n    if it < warmup_iters:\n        return learning_rate * (it + 1) / (warmup_iters + 1)\n    # 2) if it > lr_decay_iters, return min learning rate\n    if it > lr_decay_iters:\n        return min_lr\n    # 3) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n    return min_lr + coeff * (learning_rate - min_lr)"
        }
      ],
      "explanation": "A cosine decay schedule with linear warmup — a recipe borrowed from GPT-3 and Chinchilla research. Lines 233-234 handle warmup: the learning rate starts near zero and increases linearly, preventing early training instability when the model's random weights produce wild gradients. Lines 236-237 set a floor: after `lr_decay_iters`, the rate holds at `min_lr`. In between (lines 239-242), a cosine curve smoothly transitions from peak to floor. The cosine shape gives the model more time at moderate learning rates than a linear decay would."
    },
    {
      "id": "chapter-10",
      "label": "Generating Text",
      "snippets": [
        {
          "filePath": "model.py",
          "startLine": 305,
          "endLine": 330,
          "content": "    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at block_size\n            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n            # forward the model to get the logits for the index in the sequence\n            logits, _ = self(idx_cond)\n            # pluck the logits at the final step and scale by desired temperature\n            logits = logits[:, -1, :] / temperature\n            # optionally crop the logits to only the top k options\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = F.softmax(logits, dim=-1)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx"
        }
      ],
      "explanation": "Now we arrive at the payoff — the `generate` method where a trained model produces new text. The `@torch.no_grad()` decorator on line 305 tells PyTorch not to track gradients — there's nothing to learn during generation, and skipping gradient tracking saves memory and computation.\n\nThe loop on line 312 runs once per new token. Line 314 handles a practical constraint: if the sequence exceeds `block_size`, it's cropped to the most recent tokens — the model can only look back so far. Line 316 runs the full forward pass, and line 318 applies **temperature** scaling. Temperature below 1.0 makes the model more confident (sharper probability distribution), while above 1.0 makes it more creative (flatter distribution).\n\nLines 320-322 implement **top-k sampling**: only the k most likely tokens are considered, with everything else set to negative infinity before softmax. This prevents the model from occasionally picking wildly improbable tokens. Line 326 samples from the resulting probability distribution, and line 328 appends the chosen token to the sequence for the next iteration."
    },
    {
      "id": "chapter-11",
      "label": "Prompt to Output",
      "snippets": [
        {
          "filePath": "sample.py",
          "startLine": 69,
          "endLine": 89,
          "content": "else:\n    # ok let's assume gpt-2 encodings by default\n    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n    enc = tiktoken.get_encoding(\"gpt2\")\n    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n    decode = lambda l: enc.decode(l)\n\n# encode the beginning of the prompt\nif start.startswith('FILE:'):\n    with open(start[5:], 'r', encoding='utf-8') as f:\n        start = f.read()\nstart_ids = encode(start)\nx = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n\n# run generation\nwith torch.no_grad():\n    with ctx:\n        for k in range(num_samples):\n            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n            print(decode(y[0].tolist()))\n            print('---------------')"
        }
      ],
      "explanation": "With the model architecture and generation method understood, `sample.py` reveals the last mile: getting from a text prompt to readable output. Lines 69-74 set up the token decoder — if no custom `meta.pkl` exists (from character-level training), it defaults to GPT-2's BPE tokenizer via the `tiktoken` library. Between these snippets, similar logic loads a character-level encoder when one is available.\n\nLines 80-81 encode the starting prompt into token IDs and wrap them in a tensor. The generation loop (lines 84-89) is refreshingly simple: for each sample, call `model.generate` with the configured temperature and top-k values, decode the resulting token IDs back into a string, and print. Everything comes together here — from numbers back to Shakespeare (or whatever the model was trained on)."
    },
    {
      "id": "chapter-12",
      "label": "Configuration System",
      "snippets": [
        {
          "filePath": "configurator.py",
          "startLine": 20,
          "endLine": 47,
          "content": "for arg in sys.argv[1:]:\n    if '=' not in arg:\n        # assume it's the name of a config file\n        assert not arg.startswith('--')\n        config_file = arg\n        print(f\"Overriding config with {config_file}:\")\n        with open(config_file) as f:\n            print(f.read())\n        exec(open(config_file).read())\n    else:\n        # assume it's a --key=value argument\n        assert arg.startswith('--')\n        key, val = arg.split('=')\n        key = key[2:]\n        if key in globals():\n            try:\n                # attempt to eval it it (e.g. if bool, number, or etc)\n                attempt = literal_eval(val)\n            except (SyntaxError, ValueError):\n                # if that goes wrong, just use the string\n                attempt = val\n            # ensure the types match ok\n            assert type(attempt) == type(globals()[key])\n            # cross fingers\n            print(f\"Overriding: {key} = {attempt}\")\n            globals()[key] = attempt\n        else:\n            raise ValueError(f\"Unknown config key: {key}\")"
        }
      ],
      "explanation": "The `configurator.py` file is delightfully opinionated — its own docstring calls it \"Probably a terrible idea.\" It works by being `exec()`'d inside the calling script's namespace. When you pass a config file as an argument (line 28), it's literally executed as Python code, directly overriding the caller's variables. For `--key=value` arguments (lines 30-47), it parses the value with `literal_eval`, type-checks it against the existing global, and overwrites. The comment on line 43 — \"cross fingers\" — captures the spirit perfectly. No config classes, no YAML files, no argument parsers — just Python's globals dictionary and a bit of courage."
    },
    {
      "id": "chapter-13",
      "label": "The Complete Picture",
      "snippets": [],
      "explanation": "nanoGPT proves that the core ideas behind modern language models aren't nearly as complicated as they might seem. The entire pipeline follows a clear arc: raw text is tokenized into integers, those integers are embedded into vectors, stacked transformer blocks refine those vectors through attention and feedforward networks, and a final linear layer converts them back into token predictions.\n\nThe architecture rests on a few key insights we've seen throughout: **weight tying** between the input embeddings and output head saves parameters and improves coherence. **Residual connections** let blocks learn incremental refinements rather than complete transformations. **Causal masking** forces the model to only look backward, making training and generation use the same mechanism. And **multi-head attention** lets different parts of the model specialize in different kinds of relationships.\n\nOn the engineering side, Karpathy consistently chose simplicity over convention: `exec()` for configuration, `np.memmap` instead of data loaders, globals instead of config objects. The code is opinionated and occasionally hacky — but every decision makes the code easier to read and modify, which is the whole point.\n\nWhether you want to train a tiny character-level model on Shakespeare or fine-tune a full GPT-2 on your own data, you now understand every moving part. The distance from \"text goes in\" to \"text comes out\" is surprisingly short."
    }
  ]
}